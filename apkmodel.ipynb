{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"apkmodel.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOErgESwTwDhqsrApmtOM6T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exMDlTG4v9MB","executionInfo":{"status":"ok","timestamp":1614266151849,"user_tz":-300,"elapsed":1107,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"f9e20fe2-9cd0-495c-95c8-3c54ac6fe899"},"source":["!nvcc --version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2019 NVIDIA Corporation\n","Built on Sun_Jul_28_19:07:16_PDT_2019\n","Cuda compilation tools, release 10.1, V10.1.243\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7El6PTGwyId","executionInfo":{"status":"ok","timestamp":1614266186899,"user_tz":-300,"elapsed":1924,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"ef6a9ea6-5656-4c86-fd18-7cd0870299cd"},"source":["!python --version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.7.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PWb8HEWw__L","executionInfo":{"status":"ok","timestamp":1614266248635,"user_tz":-300,"elapsed":7554,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"75862e91-f784-48ae-dc76-de317a1aa993"},"source":["!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.7.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision) (0.6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxKkL9OcCGm0","executionInfo":{"status":"ok","timestamp":1627980446874,"user_tz":-300,"elapsed":33066,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"04a8365e-f5ed-4af1-80ea-88df2ccf5c9c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!ls '/content/gdrive/My Drive'\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","'Colab Notebooks'   Cryptography   gan\t GanLimited   Pretrained\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"17jx1ige6U14"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X4jIJJ8xTJt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627980529736,"user_tz":-300,"elapsed":66817,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"e54d90b3-3d3d-4a1b-82ce-6b6112ac2c88"},"source":["import os\n","\n","import torch.nn as nn\n","import torch\n","import torch.optim as optim\n","from torchsummary import summary\n","import numpy as np\n","from PIL import Image\n","\n","class Unit(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Unit, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)\n","        self.bn = nn.BatchNorm2d(num_features=out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        output = self.conv(input)\n","        output = self.bn(output)\n","        output = self.relu(output)\n","\n","        return output\n","\n","\n","class SimpleNet(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super(SimpleNet, self).__init__()\n","\n","        # Create 14 layers of the unit with max pooling in between\n","        self.unit1 = Unit(in_channels=3, out_channels=32)\n","        self.unit2 = Unit(in_channels=32, out_channels=32)\n","        self.unit3 = Unit(in_channels=32, out_channels=32)\n","\n","        self.pool1 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit4 = Unit(in_channels=32, out_channels=64)\n","        self.unit5 = Unit(in_channels=64, out_channels=64)\n","        self.unit6 = Unit(in_channels=64, out_channels=64)\n","        self.unit7 = Unit(in_channels=64, out_channels=64)\n","\n","        self.pool2 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit8 = Unit(in_channels=64, out_channels=128)\n","        self.unit9 = Unit(in_channels=128, out_channels=128)\n","        self.unit10 = Unit(in_channels=128, out_channels=128)\n","        self.unit11 = Unit(in_channels=128, out_channels=128)\n","\n","        self.pool3 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit12 = Unit(in_channels=128, out_channels=128)\n","        self.unit13 = Unit(in_channels=128, out_channels=128)\n","        self.unit14 = Unit(in_channels=128, out_channels=128)\n","\n","        self.avgpool = nn.AvgPool2d(kernel_size=4)\n","\n","        # Add all the units into the Sequential layer in exact order\n","        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6\n","                                 , self.unit7, self.pool2, self.unit8, self.unit9, self.unit10, self.unit11, self.pool3,\n","                                 self.unit12, self.unit13, self.unit14, self.avgpool)\n","\n","        self.fc = nn.Linear(in_features=128, out_features=num_classes)\n","\n","    def forward(self, input):\n","        output = self.net(input)\n","        print(output.shape)\n","        output = output.view(-1, 128)\n","        print(output.shape)\n","        output = self.fc(output)\n","        return output\n","\t\n","    def apktoimage(self,apk):\n","        with open(apk, 'r') as in_file:\n","            binary_file = np.fromfile(in_file, dtype=np.uint8)\n","            img_shape = np.ceil(np.sqrt(binary_file.shape[0])).astype(int)\n","            print(img_shape)\n","            if img_shape < 128:\n","                img_shape = 128\n","            img = np.zeros(img_shape ** 2)\n","            img[:binary_file.size] = binary_file\n","            img = img.reshape(img_shape, img_shape)\n","            img = img[:128, :128]\n","            print(img)\n","            # np.save('./malware_img'+str(ix)+'.npy',img)\n","            im = Image.fromarray(img)\n","            if im.mode != 'RGB':\n","                im = im.convert('RGB')\n","            im = im.tobitmap()\n","\n","            return im\n","\t\n","\n","\n","def save_models(epoch):\n","    torch.save(model.state_dict(), \"cifar10model_{}.model\".format(epoch))\n","    print(\"Checkpoint saved\")\n","\n","\n","def test(model, device, test_loader):\n","    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        batch_count = 0\n","        for data, target in test_loader:\n","            batch_count += 1\n","            data, target = data.to(device), target.to(device)\n","            \n","            # Get the predicted classes for this batch\n","            output = model(data)\n","            \n","            # Calculate the loss for this batch\n","            test_loss += loss_criteria(output, target).item()\n","            \n","            # Calculate the accuracy for this batch\n","            _, predicted = torch.max(output.data, 1)\n","            correct += torch.sum(target==predicted).item()\n","\n","    # Calculate the average loss and total accuracy for this epoch\n","    avg_loss = test_loss / batch_count\n","    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        avg_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    # return average loss for the epoch\n","    return avg_loss\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    # Set the model to training mode\n","    model.train()\n","    train_loss = 0\n","    print(\"Epoch:\", epoch)\n","    # Process the images in batches\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Use the CPU or GPU as appropriate\n","        # Recall that GPU is optimized for the operations we are dealing with\n","        data, target = data.to(device), target.to(device)\n","        \n","        # Reset the optimizer\n","        optimizer.zero_grad()\n","        \n","        # Push the data forward through the model layers\n","        output = model(data)\n","        \n","        # Get the loss\n","        loss = loss_criteria(output, target)\n","\n","        # Keep a running total\n","        train_loss += loss.item()\n","        \n","        # Backpropagate\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Print metrics so we see some progress\n","        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n","            \n","    # return average loss for the epoch\n","    avg_loss = train_loss / (batch_idx+1)\n","    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n","    return avg_loss\n","\n","\n","\n","def load_dataset(data_path):\n","    import torch\n","    import torchvision\n","    import torchvision.transforms as transforms\n","    # Load all the images\n","    transformation = transforms.Compose([\n","        # Randomly augment the image data\n","        # Random horizontal flip\n","        transforms.RandomCrop(32,padding=4),\n","        transforms.RandomHorizontalFlip(0.5),\n","        # Random vertical flip\n","        transforms.RandomVerticalFlip(0.3),\n","\n","        # transform to tensors\n","        transforms.ToTensor(),\n","        # Normalize the pixel values (in R, G, and B channels)\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","    # Load all of the images, transforming them\n","    full_dataset = torchvision.datasets.ImageFolder(\n","        root=data_path,transform=transformation)\n","    # Split into training (70% and testing (30%) datasets)\n","    train_size = int(0.7 * len(full_dataset))\n","    print(str(train_size))\n","    test_size = len(full_dataset) - train_size\n","    print(str(test_size))\n","\n","    # use torch.utils.data.random_split for training/test split\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","\n","    # define a loader for the training data we can iterate through in 50-image batches\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=32,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","\n","    # define a loader for the testing data we can iterate through in 50-image batches\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","\n","    return train_loader, test_loader\n","\n","\n","if __name__ == '__main__':\n","    train_folder = '/content/gdrive/My Drive/Cryptography/dataset/apk_images/'\n","    # Specify the loss criteria\n","    loss_criteria = nn.CrossEntropyLoss()\n","    if torch.cuda.is_available():\n","       device = torch.device(\"cuda\") \n","    else:\n","      device = torch.device(\"cpu\")\n","    print('Training on', device)\n","    model = SimpleNet(2).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\t  #model.summary()\n","\t\n","    train_loader, test_loader = load_dataset(train_folder)\n","    batch_size = train_loader.batch_size\n","\t  \n","    # Track metrics in these arrays\n","    epoch_nums = []\n","    training_loss = []\n","    validation_loss = []\n","\n","    # Train over 10 epochs (We restrict to 10 for time issues)\n","    epochs = 10\n","    print('Training on', device)\n","    for epoch in range(1, epochs + 1):\n","            train_loss = train(model, device, train_loader, optimizer, epoch)\n","            test_loss = test(model, device, test_loader)\n","            epoch_nums.append(epoch)\n","            training_loss.append(train_loss)\n","            validation_loss.append(test_loss)\n","\n","    print('Finished Training')\n","    PATH = '/content/gdrive/My Drive/Cryptography/mymodel.pt'\n","    # Specify the loss criteria/apkmodel.pt'\n","    torch.save(model, PATH)\n","    \n","    torchscript_model = torch.jit.script(model)\n","    #example = torch.rand(1, 3, 224, 224)\n","    from torch.utils.mobile_optimizer import optimize_for_mobile\n","    torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n","    torch.jit.save(torchscript_model_optimized, \"/content/gdrive/My Drive/Cryptography/mobilmodel.pt\")\n","\n","    #script_model.save('/content/gdrive/My Drive/Cryptography/scrip_model.pt')\n","\n","\n","    #print(script_model)\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Training on cuda\n","42\n","18\n","Training on cuda\n","Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"},{"output_type":"stream","text":["torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.698958\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 1.053939\n","Training set: Average loss: 0.876448\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 0.683226, Accuracy: 11/18 (61%)\n","\n","Epoch: 2\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 1.708117\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 1.451486\n","Training set: Average loss: 1.579801\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 3.741739, Accuracy: 6/18 (33%)\n","\n","Epoch: 3\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.821532\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.615993\n","Training set: Average loss: 0.718763\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 13.970962, Accuracy: 7/18 (39%)\n","\n","Epoch: 4\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.778071\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.580719\n","Training set: Average loss: 0.679395\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 20.565073, Accuracy: 7/18 (39%)\n","\n","Epoch: 5\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.729917\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.538209\n","Training set: Average loss: 0.634063\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 42.304478, Accuracy: 7/18 (39%)\n","\n","Epoch: 6\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.725972\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.648710\n","Training set: Average loss: 0.687341\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 51.124828, Accuracy: 7/18 (39%)\n","\n","Epoch: 7\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.842898\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.643493\n","Training set: Average loss: 0.743195\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 12.791042, Accuracy: 7/18 (39%)\n","\n","Epoch: 8\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.701693\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.620232\n","Training set: Average loss: 0.660963\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 2.980081, Accuracy: 5/18 (28%)\n","\n","Epoch: 9\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.687579\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.636937\n","Training set: Average loss: 0.662258\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 3.571846, Accuracy: 7/18 (39%)\n","\n","Epoch: 10\n","torch.Size([32, 128, 1, 1])\n","torch.Size([32, 128])\n","\tTraining batch 1 Loss: 0.626356\n","torch.Size([10, 128, 1, 1])\n","torch.Size([10, 128])\n","\tTraining batch 2 Loss: 0.493682\n","Training set: Average loss: 0.560019\n","torch.Size([18, 128, 1, 1])\n","torch.Size([18, 128])\n","Validation set: Average loss: 4.575943, Accuracy: 10/18 (56%)\n","\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-ZiRvHfVkBB","executionInfo":{"status":"ok","timestamp":1627982821633,"user_tz":-300,"elapsed":5574,"user":{"displayName":"Sayed Amin Hussain Sayed Iqbal Hussain","photoUrl":"","userId":"00433265733355207603"}},"outputId":"e208234b-2901-4cd5-f3d4-342b70eb6eba"},"source":["!pip install flask-ngrok"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-uc3rq3Wtc-"},"source":["import torch\n","import torchvision\n","\n","model = torchvision.models.resnet18(pretrained=True)\n","model.eval()\n","example = torch.rand(1, 3, 224, 224)\n","traced_script_module = torch.jit.trace(model, example)\n","traced_script_module.save(\"/content/gdrive/My Drive/Pretrained/apkmodel.pt\")"],"execution_count":null,"outputs":[]}]}